{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24db53fd",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e023eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from: D:\\programs\\astro_web\\pretrained_model\\train\\_classes.csv\n",
      "Found 1313 validated image filenames.\n",
      "Found 328 validated image filenames.\n",
      "Creating model with DenseNet201 architecture...\n",
      "Phase 1: Training top layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 198\u001b[0m\n\u001b[0;32m    195\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprograms\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mastro_web\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mpretrained_model\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     history1, history2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_images_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 141\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_csv_path, train_images_dir, output_dir)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Phase 1: Training top layers\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhase 1: Training top layers...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 141\u001b[0m history1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhase 2: Fine-tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# Unfreeze last 30% of the base model layers\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:368\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    367\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 368\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:216\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    214\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    218\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:889\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    887\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 889\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    892\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    701\u001b[0m   \u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    304\u001b[0m       placeholder_context\n\u001b[0;32m    305\u001b[0m   )\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[1;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_autograph_artifact(f):\n\u001b[0;32m    338\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPermanently allowed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph artifact\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[1;32m--> 339\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# If this is a partial, unwrap it and redo all the checks.\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, functools\u001b[38;5;241m.\u001b[39mpartial):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[0m, in \u001b[0;36mdo_not_convert.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    642\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:129\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.multi_step_on_iterator\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmulti_step_on_iterator\u001b[39m(iterator):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mOptional\u001b[38;5;241m.\u001b[39mfrom_value(\n\u001b[1;32m--> 129\u001b[0m             \u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m         )\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     empty_outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mOptional\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:889\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    887\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 889\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    892\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    701\u001b[0m   \u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    304\u001b[0m       placeholder_context\n\u001b[0;32m    305\u001b[0m   )\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[1;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_autograph_artifact(f):\n\u001b[0;32m    338\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPermanently allowed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph artifact\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[1;32m--> 339\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# If this is a partial, unwrap it and redo all the checks.\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, functools\u001b[38;5;241m.\u001b[39mpartial):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[0m, in \u001b[0;36mdo_not_convert.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    642\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:110\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 110\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m    112\u001b[0m         outputs,\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m    114\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    115\u001b[0m     )\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1673\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m   1669\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m   1672\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1673\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3263\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3261\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   3262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m-> 3263\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4061\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   4059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m   4060\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m-> 4061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[0m, in \u001b[0;36mdo_not_convert.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    642\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:56\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_has_training_arg:\n\u001b[1;32m---> 56\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\layer.py:899\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\ops\\operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[0;32m     43\u001b[0m         call_fn,\n\u001b[0;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\models\\functional.py:182\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    181\u001b[0m             backend\u001b[38;5;241m.\u001b[39mset_keras_mask(x, mask)\n\u001b[1;32m--> 182\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_through_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\ops\\function.py:171\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[1;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[0;32m    169\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39moutputs, tree\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\models\\functional.py:632\u001b[0m, in \u001b[0;36moperation_fn.<locals>.call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(operation, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_has_training_arg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m operation\u001b[38;5;241m.\u001b[39m_call_has_training_arg\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    630\u001b[0m ):\n\u001b[0;32m    631\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m training\n\u001b[1;32m--> 632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m operation(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\layer.py:899\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\ops\\operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[0;32m     43\u001b[0m         call_fn,\n\u001b[0;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:278\u001b[0m, in \u001b[0;36mBatchNormalization.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    276\u001b[0m     beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_normalization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcast(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\ops\\nn.py:2082\u001b[0m, in \u001b[0;36mbatch_normalization\u001b[1;34m(x, mean, variance, axis, offset, scale, epsilon)\u001b[0m\n\u001b[0;32m   2077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x, mean, variance, offset, scale)):\n\u001b[0;32m   2078\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BatchNorm(axis, epsilon)\u001b[38;5;241m.\u001b[39msymbolic_call(\n\u001b[0;32m   2079\u001b[0m         x, mean, variance, offset, scale\n\u001b[0;32m   2080\u001b[0m     )\n\u001b[1;32m-> 2082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_normalization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\n\u001b[0;32m   2084\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:840\u001b[0m, in \u001b[0;36mbatch_normalization\u001b[1;34m(x, mean, variance, axis, offset, scale, epsilon)\u001b[0m\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    838\u001b[0m         scale \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(scale, shape)\n\u001b[1;32m--> 840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_normalization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariance_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:1482\u001b[0m, in \u001b[0;36mbatch_normalization\u001b[1;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Batch normalization.\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m \n\u001b[0;32m   1431\u001b[0m \u001b[38;5;124;03mNormalizes a tensor by `mean` and `variance`, and applies (optionally) a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;124;03m    ([pdf](http://proceedings.mlr.press/v37/ioffe15.pdf))\u001b[39;00m\n\u001b[0;32m   1480\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatchnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m, [x, mean, variance, scale, offset]):\n\u001b[1;32m-> 1482\u001b[0m   inv \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mrsqrt(\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvariance_epsilon\u001b[49m)\n\u001b[0;32m   1483\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1484\u001b[0m     inv \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m scale\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\override_binary_operator.py:113\u001b[0m, in \u001b[0;36moverride_binary_operator_helper.<locals>.binary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m   \u001b[38;5;66;03m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[0;32m    110\u001b[0m   \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[0;32m    111\u001b[0m   \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m   x, y \u001b[38;5;241m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[1;32m--> 113\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m   \u001b[38;5;66;03m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[0;32m    116\u001b[0m   \u001b[38;5;66;03m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m   \u001b[38;5;66;03m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[0;32m    120\u001b[0m   \u001b[38;5;66;03m# informative.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(y), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__r\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m op_name):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_math_operator_overrides.py:28\u001b[0m, in \u001b[0;36m_add_dispatch_factory\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_dispatch_factory\u001b[39m(x, y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     26\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[1;32m---> 28\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmath_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1717\u001b[0m, in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1715\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops\u001b[38;5;241m.\u001b[39madd(x, y, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   1716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1717\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:519\u001b[0m, in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m--> 519\u001b[0m _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_op_def_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAddV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m _result \u001b[38;5;241m=\u001b[39m _outputs[:]\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:796\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    791\u001b[0m must_colocate_inputs \u001b[38;5;241m=\u001b[39m [val \u001b[38;5;28;01mfor\u001b[39;00m arg, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(op_def\u001b[38;5;241m.\u001b[39minput_arg, inputs)\n\u001b[0;32m    792\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m arg\u001b[38;5;241m.\u001b[39mis_ref]\n\u001b[0;32m    793\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[0;32m    794\u001b[0m   \u001b[38;5;66;03m# Add Op to graph\u001b[39;00m\n\u001b[0;32m    795\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 796\u001b[0m   op \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattr_protos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# for more details.\u001b[39;00m\n\u001b[0;32m    804\u001b[0m outputs \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39moutputs\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    668\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    669\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[1;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2701\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   2698\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   2699\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   2700\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 2701\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_node_def\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2702\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2703\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2704\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2705\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2706\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2707\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2708\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2709\u001b[0m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2710\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2711\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   2712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1196\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1193\u001b[0m     control_input_ops\u001b[38;5;241m.\u001b[39mappend(control_op)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1196\u001b[0m c_op \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_input_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Operation(c_op, SymbolicTensor)\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(g)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1026\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39m_c_graph\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mas\u001b[39;00m c_graph:\n\u001b[1;32m-> 1026\u001b[0m   op_desc \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_NewOperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node_def\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[0;32m   1030\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetDevice(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mdevice))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def create_model(num_classes):\n",
    "    base_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Enhanced feature extraction layers\n",
    "    x = Dense(2048, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    predictions = Dense(num_classes, activation='sigmoid')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Initially freeze base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "def train(train_csv_path, train_images_dir, output_dir):\n",
    "    train_csv_path = Path(train_csv_path)\n",
    "    train_images_dir = Path(train_images_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"Loading training data from: {train_csv_path}\")\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    constellation_cols = [col for col in train_df.columns if col != 'filename']\n",
    "    \n",
    "    # Enhanced data augmentation\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        zoom_range=0.3,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        brightness_range=[0.7, 1.3],\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.2,\n",
    "        shear_range=0.2\n",
    "    )\n",
    "    \n",
    "    batch_size = 16\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory=str(train_images_dir),\n",
    "        x_col='filename',\n",
    "        y_col=constellation_cols,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw',\n",
    "        subset='training'\n",
    "    )\n",
    "    \n",
    "    validation_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        directory=str(train_images_dir),\n",
    "        x_col='filename',\n",
    "        y_col=constellation_cols,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw',\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    print(\"Creating model with DenseNet201 architecture...\")\n",
    "    model, base_model = create_model(len(constellation_cols))\n",
    "    \n",
    "    # Custom learning rate schedule\n",
    "    initial_learning_rate = 0.001\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.9\n",
    "    learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps, decay_rate\n",
    "    )\n",
    "    \n",
    "    # Modified metrics for multi-label classification\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate_schedule),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', \n",
    "                tf.keras.metrics.AUC(multi_label=True),\n",
    "                tf.keras.metrics.Precision(),\n",
    "                tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    model_path = output_dir / 'constellation_model_densenet.keras'\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        str(model_path),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    log_dir = output_dir / \"logs\" / f\"densenet_{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=str(log_dir),\n",
    "        histogram_freq=1\n",
    "    )\n",
    "    \n",
    "    # Phase 1: Training top layers\n",
    "    print(\"Phase 1: Training top layers...\")\n",
    "    history1 = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=20,\n",
    "        callbacks=[checkpoint, early_stopping, reduce_lr, tensorboard_callback]\n",
    "    )\n",
    "    \n",
    "    print(\"Phase 2: Fine-tuning...\")\n",
    "    # Unfreeze last 30% of the base model layers\n",
    "    num_layers_to_unfreeze = int(len(base_model.layers) * 0.3)\n",
    "    for layer in model.layers[-num_layers_to_unfreeze:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Create a new optimizer with a lower learning rate schedule\n",
    "    new_learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=initial_learning_rate / 10,\n",
    "        decay_steps=decay_steps,\n",
    "        decay_rate=decay_rate\n",
    "    )\n",
    "    new_optimizer = Adam(learning_rate=new_learning_rate_schedule)\n",
    "\n",
    "    # Recompile the model with the new optimizer\n",
    "    model.compile(\n",
    "        optimizer=new_optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy',\n",
    "             tf.keras.metrics.AUC(multi_label=True),\n",
    "             tf.keras.metrics.Precision(),\n",
    "             tf.keras.metrics.Recall()]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history2 = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=20,\n",
    "        callbacks=[checkpoint, early_stopping, reduce_lr, tensorboard_callback]\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Save labels\n",
    "    labels_path = output_dir / 'constellation_labels.txt'\n",
    "    with open(labels_path, 'w') as f:\n",
    "        f.write('\\n'.join(constellation_cols))\n",
    "    \n",
    "    print(f\"Training completed. Model saved to: {model_path}\")\n",
    "    print(f\"Labels saved to: {labels_path}\")\n",
    "    print(f\"TensorBoard logs saved to: {log_dir}\")\n",
    "    \n",
    "    return history1, history2\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_csv_path = r\"D:\\programs\\astro_web\\pretrained_model\\train\\_classes.csv\"\n",
    "    train_images_dir = r\"D:\\programs\\astro_web\\pretrained_model\\train\"\n",
    "    output_dir = r\"D:\\programs\\astro_web\\pretrained_model\\output\"\n",
    "    \n",
    "    try:\n",
    "        history1, history2 = train(train_csv_path, train_images_dir, output_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4dfac",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c96e37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 116\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2b6b2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and labels...\n",
      "Creating validation generator...\n",
      "Found 469 validated image filenames.\n",
      "Calculating optimal thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 2s/step\n",
      "Generating predictions...\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step\n",
      "Generating metrics report...\n",
      "\n",
      "Validation Results Summary:\n",
      "       Label  Precision   Recall  F1-Score  Support  Threshold\n",
      "      aquila   0.969697 0.941176  0.955224     68.0       0.30\n",
      "      bootes   0.872093 0.986842  0.925926     76.0       0.30\n",
      " canis_major   1.000000 0.913043  0.954545     69.0       0.30\n",
      " canis_minor   0.965116 0.976471  0.970760     85.0       0.30\n",
      "  cassiopeia   0.862069 0.986842  0.920245    152.0       0.35\n",
      "      cygnus   0.862745 0.888889  0.875622     99.0       0.35\n",
      "      gemini   1.000000 0.934783  0.966292     92.0       0.40\n",
      "         leo   0.939394 0.885714  0.911765     70.0       0.65\n",
      "        lyra   0.990476 0.920354  0.954128    113.0       0.30\n",
      "        moon   1.000000 0.846847  0.917073    111.0       0.30\n",
      "       orion   0.960000 0.969697  0.964824     99.0       0.55\n",
      "    pleiades   0.866197 0.924812  0.894545    133.0       0.55\n",
      " sagittarius   1.000000 0.979167  0.989474     48.0       0.40\n",
      "    scorpius   0.950000 0.950000  0.950000     60.0       0.30\n",
      "      taurus   0.920455 0.975904  0.947368     83.0       0.35\n",
      "  ursa_major   0.944882 0.983607  0.963855    122.0       0.30\n",
      "\n",
      "Confusion Matrices:\n",
      "\n",
      " aquila:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[399   2]\n",
      " [  4  64]]\n",
      "\n",
      " bootes:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[382  11]\n",
      " [  1  75]]\n",
      "\n",
      " canis_major:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[400   0]\n",
      " [  6  63]]\n",
      "\n",
      " canis_minor:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[381   3]\n",
      " [  2  83]]\n",
      "\n",
      " cassiopeia:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[293  24]\n",
      " [  2 150]]\n",
      "\n",
      " cygnus:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[356  14]\n",
      " [ 11  88]]\n",
      "\n",
      " gemini:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[377   0]\n",
      " [  6  86]]\n",
      "\n",
      " leo:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[395   4]\n",
      " [  8  62]]\n",
      "\n",
      " lyra:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[355   1]\n",
      " [  9 104]]\n",
      "\n",
      " moon:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[358   0]\n",
      " [ 17  94]]\n",
      "\n",
      " orion:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[366   4]\n",
      " [  3  96]]\n",
      "\n",
      " pleiades:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[317  19]\n",
      " [ 10 123]]\n",
      "\n",
      " sagittarius:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[421   0]\n",
      " [  1  47]]\n",
      "\n",
      " scorpius:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[406   3]\n",
      " [  3  57]]\n",
      "\n",
      " taurus:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[379   7]\n",
      " [  2  81]]\n",
      "\n",
      " ursa_major:\n",
      "Format: [[TN, FP], [FN, TP]]\n",
      "[[340   7]\n",
      " [  2 120]]\n",
      "\n",
      "Detailed metrics saved to: D:\\programs\\astro_web\\pretrained_model\\output/validation_metrics/\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "def load_model_and_labels(model_path, labels_path):\n",
    "    \"\"\"\n",
    "    Load the trained model and corresponding class labels.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved Keras model\n",
    "        labels_path: Path to the text file containing class labels\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded Keras model\n",
    "        labels: List of class labels\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    with open(labels_path, 'r') as f:\n",
    "        labels = f.read().splitlines()\n",
    "    return model, labels\n",
    "\n",
    "def create_validation_generator(val_csv_path, val_images_dir, labels, batch_size=16):\n",
    "    \"\"\"\n",
    "    Create a data generator for validation data.\n",
    "    \n",
    "    Args:\n",
    "        val_csv_path: Path to validation CSV file\n",
    "        val_images_dir: Directory containing validation images\n",
    "        labels: List of class labels\n",
    "        batch_size: Batch size for validation\n",
    "    \n",
    "    Returns:\n",
    "        validation_generator: DataFrameIterator for validation data\n",
    "        val_df: Validation DataFrame\n",
    "    \"\"\"\n",
    "    val_df = pd.read_csv(val_csv_path)\n",
    "    \n",
    "    # Ensure all label columns exist in the validation DataFrame\n",
    "    for label in labels:\n",
    "        if label not in val_df.columns:\n",
    "            raise ValueError(f\"Label '{label}' not found in validation data\")\n",
    "    \n",
    "    validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255  # Only rescaling for validation, no augmentation\n",
    "    )\n",
    "    \n",
    "    validation_generator = validation_datagen.flow_from_dataframe(\n",
    "        dataframe=val_df,\n",
    "        directory=str(val_images_dir),\n",
    "        x_col='filename',\n",
    "        y_col=labels,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw',\n",
    "        shuffle=False  # Important for maintaining order in predictions\n",
    "    )\n",
    "    \n",
    "    return validation_generator, val_df\n",
    "\n",
    "def calculate_optimal_thresholds(model, validation_generator, labels):\n",
    "    \"\"\"\n",
    "    Calculate optimal prediction thresholds for each class using validation data.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        validation_generator: DataFrameIterator for validation data\n",
    "        labels: List of class labels\n",
    "    \n",
    "    Returns:\n",
    "        thresholds: Dictionary of optimal thresholds for each class\n",
    "    \"\"\"\n",
    "    predictions = model.predict(validation_generator)\n",
    "    true_labels = validation_generator.labels\n",
    "    \n",
    "    thresholds = {}\n",
    "    threshold_metrics = {}\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        best_metrics = None\n",
    "        \n",
    "        # Try different thresholds\n",
    "        for threshold in np.arange(0.3, 0.7, 0.05):\n",
    "            pred_binary = (predictions[:, i] > threshold).astype(int)\n",
    "            true_binary = true_labels[:, i]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            tp = np.sum((pred_binary == 1) & (true_binary == 1))\n",
    "            fp = np.sum((pred_binary == 1) & (true_binary == 0))\n",
    "            fn = np.sum((pred_binary == 0) & (true_binary == 1))\n",
    "            tn = np.sum((pred_binary == 0) & (true_binary == 0))\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "                best_metrics = {\n",
    "                    'threshold': threshold,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'tp': tp,\n",
    "                    'fp': fp,\n",
    "                    'fn': fn,\n",
    "                    'tn': tn\n",
    "                }\n",
    "        \n",
    "        thresholds[label] = best_threshold\n",
    "        threshold_metrics[label] = best_metrics\n",
    "    \n",
    "    return thresholds, threshold_metrics\n",
    "\n",
    "def generate_metrics_report(predictions, true_labels, labels, thresholds, output_dir):\n",
    "    \"\"\"\n",
    "    Generate and save detailed metrics report.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions\n",
    "        true_labels: True labels\n",
    "        labels: List of class labels\n",
    "        thresholds: Dictionary of thresholds for each class\n",
    "        output_dir: Directory to save reports\n",
    "    \"\"\"\n",
    "    # Create metrics directory\n",
    "    metrics_dir = Path(output_dir) / 'validation_metrics'\n",
    "    metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate classification report for each class\n",
    "    report_data = []\n",
    "    confusion_matrices = {}\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        pred_binary = (predictions[:, i] > thresholds[label]).astype(int)\n",
    "        true_binary = true_labels[:, i]\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        cm = confusion_matrix(true_binary, pred_binary)\n",
    "        confusion_matrices[label] = cm\n",
    "        \n",
    "        # Calculate metrics\n",
    "        report = classification_report(true_binary, pred_binary, output_dict=True)\n",
    "        report_data.append({\n",
    "            'Label': label,\n",
    "            'Precision': report['1']['precision'],\n",
    "            'Recall': report['1']['recall'],\n",
    "            'F1-Score': report['1']['f1-score'],\n",
    "            'Support': report['1']['support'],\n",
    "            'Threshold': thresholds[label]\n",
    "        })\n",
    "    \n",
    "    # Save metrics report\n",
    "    metrics_df = pd.DataFrame(report_data)\n",
    "    metrics_df.to_csv(metrics_dir / 'validation_metrics.csv', index=False)\n",
    "    \n",
    "    # Save confusion matrices in a readable format\n",
    "    with open(metrics_dir / 'confusion_matrices.txt', 'w') as f:\n",
    "        for label, cm in confusion_matrices.items():\n",
    "            f.write(f\"\\nConfusion Matrix for {label}:\\n\")\n",
    "            f.write(\"True Negative  False Positive\\n\")\n",
    "            f.write(\"False Negative True Positive\\n\")\n",
    "            f.write(str(cm))\n",
    "            f.write(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "    \n",
    "    return metrics_df, confusion_matrices\n",
    "\n",
    "def validate(model_path, val_csv_path, val_images_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Main validation function.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model\n",
    "        val_csv_path: Path to validation CSV file\n",
    "        val_images_dir: Directory containing validation images\n",
    "        output_dir: Directory to save validation results\n",
    "    \"\"\"\n",
    "    # Convert paths to Path objects\n",
    "    model_path = Path(model_path)\n",
    "    val_csv_path = Path(val_csv_path)\n",
    "    val_images_dir = Path(val_images_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # Load model and labels\n",
    "    labels_path = model_path.parent / 'constellation_labels.txt'\n",
    "    print(\"Loading model and labels...\")\n",
    "    model, labels = load_model_and_labels(model_path, labels_path)\n",
    "    \n",
    "    # Create validation generator\n",
    "    print(\"Creating validation generator...\")\n",
    "    validation_generator, val_df = create_validation_generator(val_csv_path, val_images_dir, labels)\n",
    "    \n",
    "    # Calculate optimal thresholds\n",
    "    print(\"Calculating optimal thresholds...\")\n",
    "    thresholds, threshold_metrics = calculate_optimal_thresholds(model, validation_generator, labels)\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions...\")\n",
    "    predictions = model.predict(validation_generator)\n",
    "    \n",
    "    # Generate and save metrics report\n",
    "    print(\"Generating metrics report...\")\n",
    "    metrics_df, confusion_matrices = generate_metrics_report(\n",
    "        predictions,\n",
    "        validation_generator.labels,\n",
    "        labels,\n",
    "        thresholds,\n",
    "        output_dir\n",
    "    )\n",
    "    \n",
    "    # Print summary results\n",
    "    print(\"\\nValidation Results Summary:\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nConfusion Matrices:\")\n",
    "    for label, cm in confusion_matrices.items():\n",
    "        print(f\"\\n{label}:\")\n",
    "        print(\"Format: [[TN, FP], [FN, TP]]\")\n",
    "        print(cm)\n",
    "    \n",
    "    print(f\"\\nDetailed metrics saved to: {output_dir}/validation_metrics/\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_path = r\"D:\\programs\\astro_web\\pretrained_model\\output\\constellation_model_densenet.keras\"\n",
    "    val_csv_path = r\"D:\\programs\\astro_web\\pretrained_model\\valid\\_classes.csv\"\n",
    "    val_images_dir = r\"D:\\programs\\astro_web\\pretrained_model\\valid\"\n",
    "    output_dir = r\"D:\\programs\\astro_web\\pretrained_model\\output\"\n",
    "    \n",
    "    try:\n",
    "        validate(model_path, val_csv_path, val_images_dir, output_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857ea7d",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44b1bcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model testing...\n",
      "Loading model and metadata...\n",
      "Creating test data generator...\n",
      "Found 234 validated image filenames.\n",
      "Generating predictions on test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step\n",
      "Evaluating predictions...\n",
      "Saving test results...\n",
      "\n",
      "Test Results Summary:\n",
      "       Label  Precision   Recall  F1-Score  Specificity      NPV  Support  Threshold\n",
      "      aquila   1.000000 1.000000  1.000000     1.000000 1.000000     31.0       0.30\n",
      "      bootes   0.868421 0.970588  0.916667     0.975000 0.994898     34.0       0.30\n",
      " canis_major   1.000000 0.909091  0.952381     1.000000 0.985294     33.0       0.30\n",
      " canis_minor   0.925000 1.000000  0.961039     0.984772 1.000000     37.0       0.30\n",
      "  cassiopeia   0.843750 1.000000  0.915254     0.901961 1.000000     81.0       0.35\n",
      "      cygnus   0.854545 0.870370  0.862385     0.955556 0.960894     54.0       0.35\n",
      "      gemini   0.974359 0.950000  0.962025     0.994845 0.989744     40.0       0.40\n",
      "         leo   0.888889 0.888889  0.888889     0.979798 0.979798     36.0       0.65\n",
      "        lyra   0.909091 0.961538  0.934579     0.972527 0.988827     52.0       0.30\n",
      "        moon   1.000000 0.833333  0.909091     1.000000 0.964824     42.0       0.30\n",
      "       orion   0.957447 0.918367  0.937500     0.989189 0.978610     49.0       0.55\n",
      "    pleiades   0.916667 0.891892  0.904110     0.962500 0.950617     74.0       0.55\n",
      " sagittarius   1.000000 0.850000  0.918919     1.000000 0.986175     20.0       0.40\n",
      "    scorpius   0.954545 0.954545  0.954545     0.995283 0.995283     22.0       0.30\n",
      "      taurus   0.883721 0.974359  0.926829     0.974359 0.994764     39.0       0.35\n",
      "  ursa_major   0.894737 0.985507  0.937931     0.951515 0.993671     69.0       0.30\n",
      "\n",
      "Confusion Matrices:\n",
      "\n",
      " aquila:\n",
      "True Negatives: 203\n",
      "False Positives: 0\n",
      "False Negatives: 0\n",
      "True Positives: 31\n",
      "\n",
      " bootes:\n",
      "True Negatives: 195\n",
      "False Positives: 5\n",
      "False Negatives: 1\n",
      "True Positives: 33\n",
      "\n",
      " canis_major:\n",
      "True Negatives: 201\n",
      "False Positives: 0\n",
      "False Negatives: 3\n",
      "True Positives: 30\n",
      "\n",
      " canis_minor:\n",
      "True Negatives: 194\n",
      "False Positives: 3\n",
      "False Negatives: 0\n",
      "True Positives: 37\n",
      "\n",
      " cassiopeia:\n",
      "True Negatives: 138\n",
      "False Positives: 15\n",
      "False Negatives: 0\n",
      "True Positives: 81\n",
      "\n",
      " cygnus:\n",
      "True Negatives: 172\n",
      "False Positives: 8\n",
      "False Negatives: 7\n",
      "True Positives: 47\n",
      "\n",
      " gemini:\n",
      "True Negatives: 193\n",
      "False Positives: 1\n",
      "False Negatives: 2\n",
      "True Positives: 38\n",
      "\n",
      " leo:\n",
      "True Negatives: 194\n",
      "False Positives: 4\n",
      "False Negatives: 4\n",
      "True Positives: 32\n",
      "\n",
      " lyra:\n",
      "True Negatives: 177\n",
      "False Positives: 5\n",
      "False Negatives: 2\n",
      "True Positives: 50\n",
      "\n",
      " moon:\n",
      "True Negatives: 192\n",
      "False Positives: 0\n",
      "False Negatives: 7\n",
      "True Positives: 35\n",
      "\n",
      " orion:\n",
      "True Negatives: 183\n",
      "False Positives: 2\n",
      "False Negatives: 4\n",
      "True Positives: 45\n",
      "\n",
      " pleiades:\n",
      "True Negatives: 154\n",
      "False Positives: 6\n",
      "False Negatives: 8\n",
      "True Positives: 66\n",
      "\n",
      " sagittarius:\n",
      "True Negatives: 214\n",
      "False Positives: 0\n",
      "False Negatives: 3\n",
      "True Positives: 17\n",
      "\n",
      " scorpius:\n",
      "True Negatives: 211\n",
      "False Positives: 1\n",
      "False Negatives: 1\n",
      "True Positives: 21\n",
      "\n",
      " taurus:\n",
      "True Negatives: 190\n",
      "False Positives: 5\n",
      "False Negatives: 1\n",
      "True Positives: 38\n",
      "\n",
      " ursa_major:\n",
      "True Negatives: 157\n",
      "False Positives: 8\n",
      "False Negatives: 1\n",
      "True Positives: 68\n",
      "\n",
      "Detailed results saved to: D:\\programs\\astro_web\\pretrained_model\\output/test_results/\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "def load_model_and_metadata(model_path):\n",
    "    \"\"\"\n",
    "    Load the trained model, labels, and optimal thresholds from validation.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model\n",
    "        \n",
    "    Returns:\n",
    "        model: Loaded Keras model\n",
    "        labels: List of class labels\n",
    "        thresholds: Dictionary of optimal thresholds for each class\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Load the labels\n",
    "    labels_path = Path(model_path).parent / 'constellation_labels.txt'\n",
    "    with open(labels_path, 'r') as f:\n",
    "        labels = f.read().splitlines()\n",
    "    \n",
    "    # Load optimal thresholds from validation\n",
    "    metrics_path = Path(model_path).parent / 'validation_metrics' / 'validation_metrics.csv'\n",
    "    if metrics_path.exists():\n",
    "        metrics_df = pd.read_csv(metrics_path)\n",
    "        thresholds = dict(zip(metrics_df['Label'], metrics_df['Threshold']))\n",
    "    else:\n",
    "        print(\"Warning: Validation metrics not found. Using default threshold of 0.5\")\n",
    "        thresholds = {label: 0.5 for label in labels}\n",
    "    \n",
    "    return model, labels, thresholds\n",
    "\n",
    "def create_test_generator(test_csv_path, test_images_dir, labels, batch_size=16):\n",
    "    \"\"\"\n",
    "    Create a data generator for test data.\n",
    "    \n",
    "    Args:\n",
    "        test_csv_path: Path to test CSV file\n",
    "        test_images_dir: Directory containing test images\n",
    "        labels: List of class labels\n",
    "        batch_size: Batch size for testing\n",
    "        \n",
    "    Returns:\n",
    "        test_generator: DataFrameIterator for test data\n",
    "        test_df: Test DataFrame\n",
    "        filenames: List of image filenames\n",
    "    \"\"\"\n",
    "    test_df = pd.read_csv(test_csv_path)\n",
    "    \n",
    "    # Verify all required columns exist\n",
    "    for label in labels:\n",
    "        if label not in test_df.columns:\n",
    "            raise ValueError(f\"Label '{label}' not found in test data\")\n",
    "    \n",
    "    # Create test data generator with only rescaling\n",
    "    test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255\n",
    "    )\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        directory=str(test_images_dir),\n",
    "        x_col='filename',\n",
    "        y_col=labels,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return test_generator, test_df, list(test_df['filename'])\n",
    "\n",
    "def evaluate_predictions(predictions, true_labels, labels, thresholds):\n",
    "    \"\"\"\n",
    "    Evaluate model predictions using various metrics.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions\n",
    "        true_labels: True labels\n",
    "        labels: List of class labels\n",
    "        thresholds: Dictionary of thresholds for each class\n",
    "        \n",
    "    Returns:\n",
    "        metrics_df: DataFrame containing evaluation metrics\n",
    "        confusion_matrices: Dictionary of confusion matrices for each class\n",
    "    \"\"\"\n",
    "    evaluation_results = []\n",
    "    confusion_matrices = {}\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        # Apply threshold to get binary predictions\n",
    "        pred_binary = (predictions[:, i] > thresholds[label]).astype(int)\n",
    "        true_binary = true_labels[:, i]\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        cm = confusion_matrix(true_binary, pred_binary)\n",
    "        confusion_matrices[label] = cm\n",
    "        \n",
    "        # Calculate detailed metrics\n",
    "        report = classification_report(true_binary, pred_binary, output_dict=True)\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0  # Negative Predictive Value\n",
    "        \n",
    "        evaluation_results.append({\n",
    "            'Label': label,\n",
    "            'Precision': report['1']['precision'],\n",
    "            'Recall': report['1']['recall'],\n",
    "            'F1-Score': report['1']['f1-score'],\n",
    "            'Specificity': specificity,\n",
    "            'NPV': npv,\n",
    "            'Support': report['1']['support'],\n",
    "            'Threshold': thresholds[label]\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(evaluation_results), confusion_matrices\n",
    "\n",
    "def save_test_results(metrics_df, confusion_matrices, predictions, filenames, labels, output_dir):\n",
    "    \"\"\"\n",
    "    Save all test results to files.\n",
    "    \n",
    "    Args:\n",
    "        metrics_df: DataFrame containing evaluation metrics\n",
    "        confusion_matrices: Dictionary of confusion matrices\n",
    "        predictions: Raw model predictions\n",
    "        filenames: List of image filenames\n",
    "        labels: List of class labels\n",
    "        output_dir: Directory to save results\n",
    "    \"\"\"\n",
    "    # Create test results directory\n",
    "    results_dir = Path(output_dir) / 'test_results'\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save metrics to CSV\n",
    "    metrics_df.to_csv(results_dir / 'test_metrics.csv', index=False)\n",
    "    \n",
    "    # Save confusion matrices\n",
    "    with open(results_dir / 'test_confusion_matrices.txt', 'w') as f:\n",
    "        f.write(\"Confusion Matrix Format:\\n\")\n",
    "        f.write(\"[[TN, FP],\\n [FN, TP]]\\n\\n\")\n",
    "        for label, cm in confusion_matrices.items():\n",
    "            f.write(f\"\\n{label}:\\n\")\n",
    "            f.write(str(cm))\n",
    "            f.write(\"\\n\" + \"=\"*40 + \"\\n\")\n",
    "    \n",
    "    # Save detailed predictions\n",
    "    predictions_df = pd.DataFrame(predictions, columns=labels)\n",
    "    predictions_df['filename'] = filenames\n",
    "    predictions_df.to_csv(results_dir / 'raw_predictions.csv', index=False)\n",
    "\n",
    "def test_model(model_path, test_csv_path, test_images_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Main testing function.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model\n",
    "        test_csv_path: Path to test CSV file\n",
    "        test_images_dir: Directory containing test images\n",
    "        output_dir: Directory to save test results\n",
    "    \"\"\"\n",
    "    print(\"Starting model testing...\")\n",
    "    \n",
    "    # Load model and metadata\n",
    "    print(\"Loading model and metadata...\")\n",
    "    model, labels, thresholds = load_model_and_metadata(model_path)\n",
    "    \n",
    "    # Create test generator\n",
    "    print(\"Creating test data generator...\")\n",
    "    test_generator, test_df, filenames = create_test_generator(test_csv_path, test_images_dir, labels)\n",
    "    \n",
    "    # Generate predictions\n",
    "    print(\"Generating predictions on test data...\")\n",
    "    predictions = model.predict(test_generator)\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    print(\"Evaluating predictions...\")\n",
    "    metrics_df, confusion_matrices = evaluate_predictions(\n",
    "        predictions,\n",
    "        test_generator.labels,\n",
    "        labels,\n",
    "        thresholds\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    print(\"Saving test results...\")\n",
    "    save_test_results(metrics_df, confusion_matrices, predictions, filenames, labels, output_dir)\n",
    "    \n",
    "    # Print summary results\n",
    "    print(\"\\nTest Results Summary:\")\n",
    "    print(metrics_df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nConfusion Matrices:\")\n",
    "    for label, cm in confusion_matrices.items():\n",
    "        print(f\"\\n{label}:\")\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        print(f\"True Negatives: {tn}\")\n",
    "        print(f\"False Positives: {fp}\")\n",
    "        print(f\"False Negatives: {fn}\")\n",
    "        print(f\"True Positives: {tp}\")\n",
    "    \n",
    "    print(f\"\\nDetailed results saved to: {output_dir}/test_results/\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_path = r\"D:\\programs\\astro_web\\pretrained_model\\output\\constellation_model_densenet.keras\"\n",
    "    test_csv_path = r\"D:\\programs\\astro_web\\pretrained_model\\test\\_classes.csv\"\n",
    "    test_images_dir = r\"D:\\programs\\astro_web\\pretrained_model\\test\"\n",
    "    output_dir = r\"D:\\programs\\astro_web\\pretrained_model\\output\"\n",
    "    \n",
    "    try:\n",
    "        test_model(model_path, test_csv_path, test_images_dir, output_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Testing failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e1aeea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\soure\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\soure\\anaconda3\\lib\\site-packages (9.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\soure\\anaconda3\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.65.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: rich in c:\\users\\soure\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\soure\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\soure\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2022.9.24)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.18.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\soure\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n",
      "Downloading opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "   ---------------------------------------- 0.0/38.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/38.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/38.8 MB 2.4 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 1.0/38.8 MB 2.0 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 1.3/38.8 MB 2.2 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 1.8/38.8 MB 2.1 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 2.4/38.8 MB 2.2 MB/s eta 0:00:17\n",
      "   -- ------------------------------------- 2.9/38.8 MB 2.2 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 3.4/38.8 MB 2.2 MB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 3.9/38.8 MB 2.2 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 4.5/38.8 MB 2.3 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 5.0/38.8 MB 2.3 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 5.2/38.8 MB 2.3 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 5.5/38.8 MB 2.2 MB/s eta 0:00:15\n",
      "   ----- ---------------------------------- 5.8/38.8 MB 2.1 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 6.3/38.8 MB 2.1 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 6.8/38.8 MB 2.1 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 7.3/38.8 MB 2.1 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 7.6/38.8 MB 2.1 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 7.9/38.8 MB 2.1 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 8.1/38.8 MB 2.0 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 8.9/38.8 MB 2.1 MB/s eta 0:00:15\n",
      "   --------- ------------------------------ 9.2/38.8 MB 2.1 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 9.7/38.8 MB 2.1 MB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 10.5/38.8 MB 2.2 MB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 11.3/38.8 MB 2.2 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 11.8/38.8 MB 2.2 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 12.3/38.8 MB 2.2 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 12.8/38.8 MB 2.3 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 13.1/38.8 MB 2.2 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 13.4/38.8 MB 2.2 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 13.6/38.8 MB 2.2 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 14.2/38.8 MB 2.2 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 14.7/38.8 MB 2.2 MB/s eta 0:00:12\n",
      "   --------------- ------------------------ 15.2/38.8 MB 2.2 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 15.7/38.8 MB 2.2 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 16.0/38.8 MB 2.2 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 16.5/38.8 MB 2.2 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 17.0/38.8 MB 2.2 MB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 17.3/38.8 MB 2.2 MB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 17.3/38.8 MB 2.2 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 17.6/38.8 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 17.8/38.8 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 18.1/38.8 MB 2.1 MB/s eta 0:00:11\n",
      "   ------------------ --------------------- 18.4/38.8 MB 2.0 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 18.6/38.8 MB 2.0 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 18.9/38.8 MB 2.0 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 19.4/38.8 MB 2.0 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 19.4/38.8 MB 2.0 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 19.7/38.8 MB 2.0 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 19.9/38.8 MB 2.0 MB/s eta 0:00:10\n",
      "   -------------------- ------------------- 20.2/38.8 MB 1.9 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 20.4/38.8 MB 1.9 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 20.7/38.8 MB 1.9 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 21.0/38.8 MB 1.9 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 21.0/38.8 MB 1.9 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 21.2/38.8 MB 1.9 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 21.5/38.8 MB 1.8 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 21.5/38.8 MB 1.8 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 21.8/38.8 MB 1.8 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 22.0/38.8 MB 1.8 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 22.0/38.8 MB 1.8 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 22.5/38.8 MB 1.8 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 22.5/38.8 MB 1.8 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 23.1/38.8 MB 1.7 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 23.3/38.8 MB 1.7 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 23.6/38.8 MB 1.7 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 23.9/38.8 MB 1.7 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 24.1/38.8 MB 1.7 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 24.4/38.8 MB 1.7 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 24.6/38.8 MB 1.7 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 24.9/38.8 MB 1.7 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 25.2/38.8 MB 1.7 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 25.2/38.8 MB 1.7 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 25.4/38.8 MB 1.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 25.4/38.8 MB 1.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 26.0/38.8 MB 1.7 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 26.2/38.8 MB 1.7 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 26.5/38.8 MB 1.6 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 26.7/38.8 MB 1.6 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 27.0/38.8 MB 1.6 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 27.3/38.8 MB 1.6 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 27.5/38.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 27.8/38.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 28.0/38.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 28.3/38.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 28.6/38.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 28.8/38.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 28.8/38.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 29.1/38.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 29.4/38.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 29.6/38.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 29.9/38.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 30.1/38.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 30.7/38.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 30.9/38.8 MB 1.6 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 31.5/38.8 MB 1.6 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 31.7/38.8 MB 1.6 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 32.0/38.8 MB 1.6 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 32.2/38.8 MB 1.6 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 32.8/38.8 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 33.0/38.8 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 33.3/38.8 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 33.8/38.8 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 34.1/38.8 MB 1.6 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 34.3/38.8 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 34.6/38.8 MB 1.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 34.9/38.8 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 35.1/38.8 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 35.7/38.8 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 35.7/38.8 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 35.9/38.8 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 36.2/38.8 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 36.4/38.8 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 36.7/38.8 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 37.0/38.8 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 37.2/38.8 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 37.5/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.7/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.8/38.8 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.8/38.8 MB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\soure\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow opencv-python pillow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea70c3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\soure\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 982, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 714, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 392, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\soure\\AppData\\Local\\Temp\\ipykernel_14376\\4004839033.py\", line 1, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 467, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 2, in <module>\n",
      "    from keras.api import DTypePolicy\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\api\\__init__.py\", line 8, in <module>\n",
      "    from keras.api import activations\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\api\\activations\\__init__.py\", line 7, in <module>\n",
      "    from keras.src.activations import deserialize\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\__init__.py\", line 1, in <module>\n",
      "    from keras.src import activations\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\activations\\__init__.py\", line 30, in <module>\n",
      "    from keras.src.saving import object_registration\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\saving\\__init__.py\", line 7, in <module>\n",
      "    from keras.src.saving.saving_api import load_model\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\", line 7, in <module>\n",
      "    from keras.src.legacy.saving import legacy_h5_format\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\", line 13, in <module>\n",
      "    from keras.src.legacy.saving import saving_utils\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\", line 10, in <module>\n",
      "    from keras.src import models\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\models\\__init__.py\", line 1, in <module>\n",
      "    from keras.src.models.functional import Functional\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 16, in <module>\n",
      "    from keras.src.models.model import Model\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\models\\model.py\", line 12, in <module>\n",
      "    from keras.src.trainers import trainer as base_trainer\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 14, in <module>\n",
      "    from keras.src.trainers.data_adapters import data_adapter_utils\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py\", line 4, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_data_adapter\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py\", line 7, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_slicing\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py\", line 12, in <module>\n",
      "    import pandas\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\", line 80, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\Users\\soure\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 982, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 714, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 392, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2995, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3194, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3373, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3433, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\soure\\AppData\\Local\\Temp\\ipykernel_14376\\4004839033.py\", line 1, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 467, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\__init__.py\", line 2, in <module>\n",
      "    from keras.api import DTypePolicy\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\api\\__init__.py\", line 8, in <module>\n",
      "    from keras.api import activations\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\api\\activations\\__init__.py\", line 7, in <module>\n",
      "    from keras.src.activations import deserialize\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\__init__.py\", line 1, in <module>\n",
      "    from keras.src import activations\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\activations\\__init__.py\", line 30, in <module>\n",
      "    from keras.src.saving import object_registration\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\saving\\__init__.py\", line 7, in <module>\n",
      "    from keras.src.saving.saving_api import load_model\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\saving\\saving_api.py\", line 7, in <module>\n",
      "    from keras.src.legacy.saving import legacy_h5_format\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py\", line 13, in <module>\n",
      "    from keras.src.legacy.saving import saving_utils\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\legacy\\saving\\saving_utils.py\", line 10, in <module>\n",
      "    from keras.src import models\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\models\\__init__.py\", line 1, in <module>\n",
      "    from keras.src.models.functional import Functional\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\models\\functional.py\", line 16, in <module>\n",
      "    from keras.src.models.model import Model\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\models\\model.py\", line 12, in <module>\n",
      "    from keras.src.trainers import trainer as base_trainer\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 14, in <module>\n",
      "    from keras.src.trainers.data_adapters import data_adapter_utils\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py\", line 4, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_data_adapter\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py\", line 7, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_slicing\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py\", line 12, in <module>\n",
      "    import pandas\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\", line 80, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\soure\\anaconda3\\lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 189ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "\n",
    "class ConstellationDetector:\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"\n",
    "        Initialize the constellation detector with the trained model.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the trained model file\n",
    "        \"\"\"\n",
    "        # Load the model and labels\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Load labels and thresholds\n",
    "        model_dir = Path(model_path).parent\n",
    "        with open(model_dir / 'constellation_labels.txt', 'r') as f:\n",
    "            self.labels = f.read().splitlines()\n",
    "            \n",
    "        # Load thresholds if available, otherwise use default\n",
    "        metrics_path = model_dir / 'validation_metrics' / 'validation_metrics.csv'\n",
    "        if metrics_path.exists():\n",
    "            import pandas as pd\n",
    "            metrics_df = pd.read_csv(metrics_path)\n",
    "            self.thresholds = dict(zip(metrics_df['Label'], metrics_df['Threshold']))\n",
    "        else:\n",
    "            self.thresholds = {label: 0.5 for label in self.labels}\n",
    "        \n",
    "        # Initialize the GUI\n",
    "        self.setup_gui()\n",
    "    \n",
    "    def setup_gui(self):\n",
    "        \"\"\"Set up the graphical user interface.\"\"\"\n",
    "        self.root = tk.Tk()\n",
    "        self.root.title(\"Constellation Detector\")\n",
    "        self.root.geometry(\"1200x800\")\n",
    "        \n",
    "        # Create main frame\n",
    "        main_frame = tk.Frame(self.root)\n",
    "        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Create buttons frame\n",
    "        button_frame = tk.Frame(main_frame)\n",
    "        button_frame.pack(fill=tk.X, pady=(0, 10))\n",
    "        \n",
    "        # Add buttons\n",
    "        tk.Button(button_frame, text=\"Load Image\", command=self.load_image).pack(side=tk.LEFT, padx=5)\n",
    "        tk.Button(button_frame, text=\"Detect Constellations\", command=self.process_image).pack(side=tk.LEFT, padx=5)\n",
    "        \n",
    "        # Create image frame\n",
    "        self.image_frame = tk.Frame(main_frame)\n",
    "        self.image_frame.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Create results frame\n",
    "        self.results_frame = tk.Frame(main_frame)\n",
    "        self.results_frame.pack(fill=tk.X, pady=(10, 0))\n",
    "        \n",
    "        self.image_label = tk.Label(self.image_frame)\n",
    "        self.image_label.pack()\n",
    "        \n",
    "        self.results_text = tk.Text(self.results_frame, height=5)\n",
    "        self.results_text.pack(fill=tk.X)\n",
    "        \n",
    "        self.current_image = None\n",
    "        self.current_image_path = None\n",
    "    \n",
    "    def load_image(self):\n",
    "        \"\"\"Open a file dialog to select an image.\"\"\"\n",
    "        file_path = filedialog.askopenfilename(\n",
    "            filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp *.gif *.tiff\")]\n",
    "        )\n",
    "        if file_path:\n",
    "            self.current_image_path = file_path\n",
    "            self.display_image(file_path)\n",
    "    \n",
    "    def display_image(self, image_path, highlighted_image=None):\n",
    "        \"\"\"\n",
    "        Display the image in the GUI.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the image file\n",
    "            highlighted_image: Optional numpy array of highlighted image\n",
    "        \"\"\"\n",
    "        if highlighted_image is not None:\n",
    "            # Convert BGR to RGB for PIL\n",
    "            image = cv2.cvtColor(highlighted_image, cv2.COLOR_BGR2RGB)\n",
    "            image = Image.fromarray(image)\n",
    "        else:\n",
    "            image = Image.open(image_path)\n",
    "        \n",
    "        # Resize image to fit the window while maintaining aspect ratio\n",
    "        display_size = (800, 600)\n",
    "        image.thumbnail(display_size, Image.ANTIALIAS)\n",
    "\n",
    "        \n",
    "        # Convert to PhotoImage for tkinter\n",
    "        photo = ImageTk.PhotoImage(image)\n",
    "        self.image_label.configure(image=photo)\n",
    "        self.image_label.image = photo  # Keep a reference\n",
    "    \n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Preprocess the image for model input.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the image file\n",
    "            \n",
    "        Returns:\n",
    "            processed_image: Preprocessed image ready for model input\n",
    "            original_image: Original image for visualization\n",
    "        \"\"\"\n",
    "        # Read and resize image\n",
    "        image = cv2.imread(image_path)\n",
    "        original_image = image.copy()\n",
    "        \n",
    "        # Resize to model input size\n",
    "        processed_image = cv2.resize(image, (224, 224))\n",
    "        processed_image = processed_image.astype('float32') / 255.0\n",
    "        \n",
    "        return np.expand_dims(processed_image, axis=0), original_image\n",
    "    \n",
    "    def process_image(self):\n",
    "        \"\"\"Process the loaded image and display results.\"\"\"\n",
    "        if self.current_image_path is None:\n",
    "            messagebox.showerror(\"Error\", \"Please load an image first\")\n",
    "            return\n",
    "        \n",
    "        # Preprocess image\n",
    "        input_tensor, original_image = self.preprocess_image(self.current_image_path)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.model.predict(input_tensor)\n",
    "        \n",
    "        # Get detected constellations\n",
    "        detected_constellations = []\n",
    "        for i, label in enumerate(self.labels):\n",
    "            if predictions[0][i] > self.thresholds[label]:\n",
    "                confidence = predictions[0][i] * 100\n",
    "                detected_constellations.append((label, confidence))\n",
    "        \n",
    "        # Highlight detected constellations\n",
    "        highlighted_image = self.highlight_constellations(original_image, detected_constellations)\n",
    "        \n",
    "        # Update display\n",
    "        self.display_image(self.current_image_path, highlighted_image)\n",
    "        \n",
    "        # Update results text\n",
    "        self.results_text.delete(1.0, tk.END)\n",
    "        if detected_constellations:\n",
    "            results = \"Detected Constellations:\\n\"\n",
    "            for const, conf in detected_constellations:\n",
    "                results += f\"{const}: {conf:.1f}% confidence\\n\"\n",
    "        else:\n",
    "            results = \"No constellations detected in the image.\"\n",
    "        self.results_text.insert(tk.END, results)\n",
    "    \n",
    "    def highlight_constellations(self, image, detected_constellations):\n",
    "        \"\"\"\n",
    "        Add visual highlights and labels for detected constellations.\n",
    "        \n",
    "        Args:\n",
    "            image: Original image\n",
    "            detected_constellations: List of (constellation_name, confidence) tuples\n",
    "            \n",
    "        Returns:\n",
    "            highlighted_image: Image with highlights and labels\n",
    "        \"\"\"\n",
    "        highlighted_image = image.copy()\n",
    "        \n",
    "        # Add semi-transparent overlay\n",
    "        overlay = highlighted_image.copy()\n",
    "        \n",
    "        # Add text with detected constellations\n",
    "        height = highlighted_image.shape[0]\n",
    "        for i, (const, conf) in enumerate(detected_constellations):\n",
    "            # Position text at the top of the image\n",
    "            y_position = 30 + (i * 30)\n",
    "            cv2.putText(\n",
    "                highlighted_image,\n",
    "                f\"{const} ({conf:.1f}%)\",\n",
    "                (10, y_position),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7,\n",
    "                (0, 255, 0),\n",
    "                2\n",
    "            )\n",
    "        \n",
    "        # Add subtle highlighting effect\n",
    "        cv2.addWeighted(overlay, 0.2, highlighted_image, 0.8, 0, highlighted_image)\n",
    "        \n",
    "        return highlighted_image\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Start the application.\"\"\"\n",
    "        self.root.mainloop()\n",
    "\n",
    "def main():\n",
    "    # Set up paths\n",
    "    model_path = r\"D:\\programs\\astro_web\\pretrained_model\\output\\constellation_model_densenet.keras\"\n",
    "    \n",
    "    try:\n",
    "        # Create and run the detector\n",
    "        detector = ConstellationDetector(model_path)\n",
    "        detector.run()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16212e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "Output saved to D:/programs/astro_web/pretrained_model/test/orion_constellation_043_png_jpg.rf.00830d963a648aa59f7964a6ddb65714_output.jpg\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tkinter import filedialog, Tk\n",
    "import os\n",
    "\n",
    "class ConstellationDetector:\n",
    "    def __init__(self, model_path):\n",
    "        \"\"\"\n",
    "        Initialize the constellation detector with the trained model.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the trained model file\n",
    "        \"\"\"\n",
    "        # Load the model and labels\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        \n",
    "        # Load labels and thresholds\n",
    "        model_dir = Path(model_path).parent\n",
    "        with open(model_dir / 'constellation_labels.txt', 'r') as f:\n",
    "            self.labels = f.read().splitlines()\n",
    "            \n",
    "        # Load thresholds if available, otherwise use default\n",
    "        metrics_path = model_dir / 'validation_metrics' / 'validation_metrics.csv'\n",
    "        if metrics_path.exists():\n",
    "            import pandas as pd\n",
    "            metrics_df = pd.read_csv(metrics_path)\n",
    "            self.thresholds = dict(zip(metrics_df['Label'], metrics_df['Threshold']))\n",
    "        else:\n",
    "            self.thresholds = {label: 0.5 for label in self.labels}\n",
    "    \n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Preprocess the image for model input.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the image file\n",
    "            \n",
    "        Returns:\n",
    "            processed_image: Preprocessed image ready for model input\n",
    "            original_image: Original image for visualization\n",
    "        \"\"\"\n",
    "        # Read and resize image\n",
    "        image = cv2.imread(image_path)\n",
    "        original_image = image.copy()\n",
    "        \n",
    "        # Resize to model input size\n",
    "        processed_image = cv2.resize(image, (224, 224))\n",
    "        processed_image = processed_image.astype('float32') / 255.0\n",
    "        \n",
    "        return np.expand_dims(processed_image, axis=0), original_image\n",
    "    \n",
    "    def process_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Process the loaded image and save the result with highlighted constellations.\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the input image\n",
    "        \"\"\"\n",
    "        # Preprocess image\n",
    "        input_tensor, original_image = self.preprocess_image(image_path)\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = self.model.predict(input_tensor)\n",
    "        \n",
    "        # Get detected constellations\n",
    "        detected_constellations = []\n",
    "        for i, label in enumerate(self.labels):\n",
    "            if predictions[0][i] > self.thresholds[label]:\n",
    "                confidence = predictions[0][i] * 100\n",
    "                detected_constellations.append((label, confidence))\n",
    "        \n",
    "        # Highlight detected constellations\n",
    "        highlighted_image = self.highlight_constellations(original_image, detected_constellations)\n",
    "        \n",
    "        # Save the output image\n",
    "        output_path = os.path.splitext(image_path)[0] + \"_output.jpg\"\n",
    "        cv2.imwrite(output_path, highlighted_image)\n",
    "        print(f\"Output saved to {output_path}\")\n",
    "    \n",
    "    def highlight_constellations(self, image, detected_constellations):\n",
    "        \"\"\"\n",
    "        Add visual highlights and labels for detected constellations.\n",
    "        \n",
    "        Args:\n",
    "            image: Original image\n",
    "            detected_constellations: List of (constellation_name, confidence) tuples\n",
    "            \n",
    "        Returns:\n",
    "            highlighted_image: Image with highlights and labels\n",
    "        \"\"\"\n",
    "        highlighted_image = image.copy()\n",
    "        \n",
    "        # Add text with detected constellations\n",
    "        for i, (const, conf) in enumerate(detected_constellations):\n",
    "            # Position text at the top of the image\n",
    "            y_position = 30 + (i * 30)\n",
    "            cv2.putText(\n",
    "                highlighted_image,\n",
    "                f\"{const} ({conf:.1f}%)\",\n",
    "                (10, y_position),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7,\n",
    "                (0, 255, 0),\n",
    "                2\n",
    "            )\n",
    "        \n",
    "        return highlighted_image\n",
    "\n",
    "def main():\n",
    "    # Set up paths\n",
    "    model_path = r\"D:\\programs\\astro_web\\pretrained_model\\output\\constellation_model_densenet.keras\"\n",
    "    \n",
    "    # File selection\n",
    "    root = Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select an image\",\n",
    "        filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png *.bmp *.tiff\")]\n",
    "    )\n",
    "    if not file_path:\n",
    "        print(\"No file selected. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Create and run the detector\n",
    "    try:\n",
    "        detector = ConstellationDetector(model_path)\n",
    "        detector.process_image(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54372585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
